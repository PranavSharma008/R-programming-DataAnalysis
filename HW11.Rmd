# Intro to Data Science - HW 11
##### Copyright 2021, Jeffrey Stanton, Jeffrey Saltz, Christopher Dunham, and Jasmina Tacheva


```{r}
# Enter your name here: Pranav Sharma
```

### Attribution statement: (choose only one and delete the rest)


```{r}
# 1. I did this homework by myself, with help from the book and the professor.
```

**Text mining** plays an important role in many industries because of the prevalence of text in the interactions between customers and company representatives. Even when the customer interaction is by speech, rather than by chat or email, speech to text algorithms have gotten so good that transcriptions of these spoken word interactions are often available. To an increasing extent, a data scientist needs to be able to wield tools that turn a body of text into actionable insights. In this homework, we explore a real **City of Syracuse dataset** using the **quanteda** and **quanteda.textplots** packages. Make sure to install the **quanteda** and **quanteda.textplots** packages before following the steps below:<br>

## Part 1: Load and visualize the data file  
A.	Take a look at this article: https://samedelstein.medium.com/snowplow-naming-contest-data-2dcd38272caf and write a comment in your R script, briefly describing what it is about.<br>


```{r}
library(quanteda)
library(quanteda.textplots)
# from a cursory look I can see that this is an article which is about snowplow naming contest. It describes what happened in the snowplow naming contest. It is also talking about past contests
```

B.	Read the data from the following URL into a dataframe called **df**:
https://intro-datascience.s3.us-east-2.amazonaws.com/snowplownames.csv


```{r}
library(tidyverse)
df <- read_csv("https://intro-datascience.s3.us-east-2.amazonaws.com/snowplownames.csv")
```

C.	Inspect the **df** dataframe â€“ which column contains an explanation of the meaning of each submitted snowplow name? 


```{r}
summary(df)
# df$meaning contains the meaning of each submitted snowplow time
df$submission_number[is.na(df$meaning)]
```

D. Transform that column into a **document-feature matrix**, using the **corpus()**, **tokens(), tokens_select()**, and **dfm()** functions from the quanteda package. Do not forget to **remove stop words**.


```{r}
#install.packages("quanteda")
library(quanteda)
# we don't have to do replace na as it is replaces by an empty string itself when using corpus
dfCorpus <- corpus(df$meaning, docnames=df$submission_number)
df_toks <- tokens(dfCorpus, remove_punct=TRUE)
# removal of the stop words
df_nostop <- tokens_select(df_toks, pattern = stopwords("en"), selection = "remove")
# conversion into a DFM
df_dfm <- dfm(df_nostop, tolower = TRUE ) 

```

E.	Plot a **word cloud** where a word is only represented if it appears **at least 2 times** in the corpus. **Hint:** use **textplot_wordcloud()** from the quanteda.textplots package:


```{r}
#install.packages("quanteda.textplots")
library(quanteda.textplots)
# df_dfm is our dfm with min_count set to 2 as asked in the question , as it should represent atleast twice
textplot_wordcloud(df_dfm, min_count = 2)

```

F.	Next, **increase the minimum count to 10**. What happens to the word cloud? **Explain in a comment**. 


```{r}
textplot_wordcloud(df_dfm, min_count = 10)
# The Number of words decrease in the word cloud thus the word cloud gets smaller. Though the size of the words do not change. I am here talking about the size of the maximum occuring words.
```

G.	What are the top 10 words in the word cloud?

**Hint**: use textstat_frequency in the quanteda.textstats package


```{r}
#install.packages("quanteda.textstats")
library(quanteda.textstats)
library(quanteda)
# using the min10 we get the top 10 most frequent words since the frequency table is sorted 
textstat_frequency(df_dfm,min(10))
```

H.	Explain in a comment what you observed in the sorted list of word counts. 


```{r}
# feature,frequency, rank, docfreq, group. Snow and syracuse are the most frequent words used in syracuse.
```

## Part 2: Analyze the sentiment of the descriptions

###Match the review words with positive and negative words

I.	Read in the list of positive words (using the scan() function), and output the first 5 words in the list. 

https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt
<br>

There should be 2006 positive words words, so you may need to clean up these lists a bit. 


```{r}
URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt"
posWords <- scan(URL, character(0), sep = "\n")
posWords <- posWords[-1:-34]
head(posWords)
```

J. Do the same for the  the negative words list (there are 4783 negative words): <br>
<br>
https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt <br>




```{r}
URL_negative <- "https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt"
negWords <- scan(URL_negative, character(0), sep = "\n")
# negative words
negWords <- negWords[-1:-34]
head(negWords)
```

J.	Using **dfm_match()** with the dfm and the positive word file you read in, and then **textstat_frequency()**, output the 10 most frequent positive words


```{r}
# matching positive words to your dfm
posDFM <- dfm_match(df_dfm, posWords) 
# display the frequency of your positive words
posFreq <- textstat_frequency(posDFM)

posFreq[1:10,]
length(posWords)
```

M.	Use R to print out the total number of positive words in the name explanation.


```{r}
# sum of frequency in the posFreq data
sum(posFreq$frequency)
```

N.	Repeat that process for the negative words you matched. Which negative words were in the name explanation variable, and what is their total number?


```{r}
# matching negative words to your dfm
negDFM <- dfm_match(df_dfm, negWords) 

# display the frequency of your negative words
negFreq <- textstat_frequency(negDFM)

sum(negFreq$frequency)
```

O.	Write a comment describing what you found after exploring the positive and negative word lists. Which group is more common in this dataset?


```{r}
#The number of positive words are more as compared to the number of negative words. Number of positive words are 866 while the number of negative words are 255. Thus the group of positive words are more common in the dataset
```

X. Complete the function below, so that it returns a sentiment score (number of positive words - number of negative words)


```{r}
doMySentiment <- function(posWords, negWords, stringToAnalyze){
  string_tok=tokens(stringToAnalyze)
  dfm_string<-dfm(string_tok, tolower = TRUE)
 pos_frequency <- 0
 neg_frequency <-0
  positive_dfm <- dfm_match(dfm_string,posWords)
  # using this to check if empty or not
  if(!is_empty(positive_dfm@x)){
  positive_freq <- textstat_frequency(positive_dfm)
  pos_frequency <- sum(positive_freq$frequency)
  }
  
  negative_dfm <- dfm_match(dfm_string,negWords)
  # using this to check if empty or not
  if(!is_empty(negative_dfm@x)){
  negative_freq<- textstat_frequency(negative_dfm)
  neg_frequency<- sum(negative_freq$frequency)
  }
  # calculating this for the sentiment score
  sentimentScore <-  pos_frequency - neg_frequency
  
  return(sentimentScore)
}
  

```

X. Test your function with the string "This book is horrible"


```{r}
doMySentiment(posWords, negWords, "This book is horrible")
```

Use the syuzhet package, to calculate the sentiment of the same phrase ("This book is horrible"), using syuzhet's **get_sentiment()** function, using the afinn method. In AFINN, words are scored as integers from -5 to +5:



```{r}
#install.packages("syuzhet")
library(syuzhet)

get_sentiment("This book is horrible", method="afinn")
```
